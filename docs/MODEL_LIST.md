# Complete Model List

**Date:** Friday, December 5, 2025  
**Total Models:** 88

---

## Summary by Provider

| Provider | Models | Free Tier | Streaming |
|----------|--------|-----------|-----------|
| Cerebras | 6 | âœ… Yes | âœ… Yes |
| Groq | 8 | âœ… Yes | âœ… Yes |
| Mistral | 13 | Trial | âœ… Yes |
| Cohere | 5 | âœ… Yes | âœ… Yes |
| NVIDIA | 56 | âœ… Yes | âœ… Yes |
| **Total** | **88** | - | **88/88** |

---

## Cerebras (6 models) - FREE TIER âœ…

All models support streaming âœ…

1. **zai-glm-4.6** [super] [64K context]
   - 357B total (32B active)
   - Dual-mode reasoning
   - ~1000 tokens/sec

2. **qwen-3-235b-a22b-instruct-2507** [super] [65K context]
   - World's fastest frontier reasoning
   - ~1400-1700 tokens/sec

3. **gpt-oss-120b** [super] [medium context]
   - Open-source GPT alternative
   - Strong reasoning

4. **llama-3.3-70b** [super] [medium context]
   - Meta's latest
   - Excellent quality

5. **qwen-3-32b** [regular] [medium context]
   - Fast and efficient
   - Good for general tasks

6. **llama3.1-8b** [regular] [small context]
   - Fastest model
   - Great for simple tasks

---

## Groq (8 models) - FREE TIER âœ…

All models support streaming âœ…

1. **moonshotai/kimi-k2-instruct-0905** [super] [128K context]
   - Excellent reasoning
   - Large context window

2. **openai/gpt-oss-120b** [super] [128K context]
   - Open-source GPT
   - Strong capabilities

3. **llama-3.3-70b-versatile** [super] [128K context]
   - Meta's versatile model
   - Great all-rounder

4. **meta-llama/llama-4-maverick-17b-128e-instruct** [regular] [131K context]
   - Vision support
   - 128 experts MoE

5. **meta-llama/llama-4-scout-17b-16e-instruct** [regular] [131K context]
   - Vision support
   - 16 experts MoE

6. **qwen/qwen3-32b** [regular] [128K context]
   - Fast and efficient
   - Multilingual

7. **openai/gpt-oss-20b** [regular] [128K context]
   - Smaller GPT-OSS
   - Good balance

8. **llama-3.1-8b-instant** [regular] [128K context]
   - Fastest on Groq
   - Instant responses

---

## Mistral (13 models) - TRIAL CREDITS

All models support streaming âœ…

### Large Models (Super)
1. **mistral-large-2411** [super] [128K context]
   - Flagship model
   - Best quality

2. **mistral-medium-2508** [super] [128K context]
   - Balanced performance
   - Good value

3. **magistral-medium-2507** [super] [128K context]
   - Reasoning specialist
   - Optimal <40K tokens

4. **pixtral-large-2411** [super] [128K context]
   - Vision support
   - Large multimodal

5. **devstral-medium-2507** [super] [medium context]
   - Code specialist
   - Development focused

### Regular Models
6. **codestral-2501** [regular] [256K context]
   - Code generation
   - Huge context

7. **magistral-small-2507** [regular] [128K context]
   - Reasoning
   - Smaller version

8. **mistral-small-2409** [regular] [128K context]
   - General purpose
   - Cost-effective

9. **pixtral-12b-2409** [regular] [128K context]
   - Vision support
   - Compact multimodal

10. **devstral-small-2505** [regular] [medium context]
    - Code specialist
    - Smaller version

11. **open-mistral-nemo** [regular] [128K context]
    - Open-source
    - 12B parameters

12. **ministral-8b-2410** [regular] [128K context]
    - Edge deployment
    - Very fast

13. **ministral-3b-2410** [regular] [128K context]
    - Edge deployment
    - Ultra-fast

---

## Cohere (5 models) - FREE TIER âœ…

All models support streaming âœ…

1. **command-r-plus-08-2024** [super] [128K context]
   - Latest flagship
   - Best quality

2. **command-r-plus** [super] [128K context]
   - Flagship model
   - RAG optimized

3. **command-r-08-2024** [regular] [128K context]
   - Latest regular
   - Good balance

4. **command-r** [regular] [128K context]
   - RAG optimized
   - Cost-effective

5. **command** [regular] [4K context]
   - Basic model
   - Simple tasks

---

## NVIDIA (56 models) - FREE TIER âœ…

All models support streaming âœ…

### Super Models (70B+)
1. **qwen/qwen3-235b-a22b** [235B]
2. **qwen/qwen3-coder-480b-a35b-instruct** [480B]
3. **qwen/qwen3-next-80b-a3b-instruct** [80B]
4. **deepseek-ai/deepseek-v3.1** [671B]
5. **deepseek-ai/deepseek-v3.1-terminus** [671B]
6. **stockmark/stockmark-2-100b-instruct** [100B]
7. **abacusai/dracarys-llama-3.1-70b-instruct** [70B]
8. **bytedance/seed-oss-36b-instruct** [36B]

### Vision Models
9. **microsoft/phi-3.5-vision-instruct**
10. **microsoft/phi-4-multimodal-instruct**
11. **nvidia/vila**
12. **meta/llama-4-maverick-17b-128e-instruct**
13. **meta/llama-4-scout-17b-16e-instruct**

### Reasoning Models
14. **microsoft/phi-4-mini-flash-reasoning**
15. **deepseek-ai/deepseek-r1-0528**
16. **qwen/qwq-32b**

### Code Specialists
17. **qwen/qwen2.5-coder-7b-instruct**
18. **mistralai/mamba-codestral-7b-v0.1**

### Multilingual Models
19. **nvidia/nemotron-4-mini-hindi-4b-instruct**
20. **nvidia/riva-translate-4b-instruct**
21. **speakleash/bielik-11b-v2.6-instruct** (Polish)
22. **igenius/italia_10b_instruct_16k** (Italian)
23. **marin/marin-8b-instruct** (Japanese)
24. **mediatek/breeze-7b-instruct** (Chinese)

### Regular Models (7B-32B)
25. **google/gemma-3-27b-it**
26. **google/gemma-3n-e4b-it**
27. **google/gemma-3n-e2b-it**
28. **google/gemma-2-27b-it**
29. **google/gemma-2-2b-it**
30. **google/gemma-7b**
31. **google/shieldgemma-9b**
32. **microsoft/phi-3-medium-128k-instruct**
33. **microsoft/phi-3-medium-4k-instruct**
34. **microsoft/phi-3-small-128k-instruct**
35. **microsoft/phi-3-small-8k-instruct**
36. **microsoft/phi-3.5-mini-instruct**
37. **mistralai/mistral-medium-3-instruct**
38. **mistralai/mistral-small-3.1-24b-instruct-2503**
39. **mistralai/magistral-small-2506**
40. **mistralai/mistral-nemotron**
41. **moonshotai/kimi-k2-instruct**
42. **moonshotai/kimi-k2-instruct-0905**
43. **minimaxai/minimax-m2**
44. **ai21labs/jamba-1.5-mini-instruct**
45. **baichuan-inc/baichuan2-13b-chat**
46. **ibm/granite-3.3-8b-instruct**
47. **ibm/granite-guardian-3.0-8b**
48. **nvidia/llama3-chatqa-1.5-8b**
49. **nvidia/nemotron-mini-4b-instruct**
50. **qwen/qwen2-7b-instruct**
51. **rakuten/rakutenai-7b-chat**
52. **rakuten/rakutenai-7b-instruct**
53. **thudm/chatglm3-6b**
54. **tiiuae/falcon3-7b-instruct**
55. **upstage/solar-10.7b-instruct**
56. **meta/llama-guard-4-12b** (Safety model)

---

## Model Selection Guide

### For Speed ðŸš€
- **Cerebras:** llama3.1-8b, qwen-3-32b
- **Groq:** llama-3.1-8b-instant
- **Mistral:** ministral-3b-2410, ministral-8b-2410

### For Quality ðŸŽ¯
- **Cerebras:** zai-glm-4.6, qwen-3-235b
- **Groq:** kimi-k2, gpt-oss-120b
- **Mistral:** mistral-large-2411
- **NVIDIA:** deepseek-v3.1, qwen3-235b

### For Vision ðŸ‘ï¸
- **Groq:** llama-4-maverick, llama-4-scout
- **Mistral:** pixtral-large-2411, pixtral-12b-2409
- **NVIDIA:** phi-3.5-vision, phi-4-multimodal, vila

### For Reasoning ðŸ§ 
- **Cerebras:** zai-glm-4.6, qwen-3-235b
- **Groq:** kimi-k2
- **Mistral:** magistral-medium, magistral-small
- **NVIDIA:** deepseek-r1, qwq-32b, phi-4-mini-flash

### For Code ðŸ’»
- **Mistral:** codestral-2501, devstral-medium
- **NVIDIA:** qwen2.5-coder, mamba-codestral

### For Large Context ðŸ“š
- **Groq:** All models (128K+)
- **Mistral:** codestral-2501 (256K)
- **NVIDIA:** Most models (128K+)

### For Free Tier ðŸ†“
- **Cerebras:** All 6 models
- **Groq:** All 8 models
- **Cohere:** All 5 models
- **NVIDIA:** All 56 models

---

## Streaming Support

### Fully Supported âœ…
- **Cerebras:** All 6 models
- **Groq:** All 8 models
- **Mistral:** All 13 models
- **Cohere:** All 5 models (custom JSON lines format)
- **NVIDIA:** All 56 models

**Total Streaming:** 88/88 models (100%)

---

## Context Windows

### Large (100K+)
- **Groq:** All models (128K-131K)
- **Mistral:** Most models (128K-256K)
- **NVIDIA:** Most models (128K+)

### Medium (30K-100K)
- **Cerebras:** Most models (64K-65K)
- **Cohere:** command-r models (128K)

### Small (<30K)
- **Cerebras:** llama3.1-8b
- **Cohere:** command (4K)

---

## Power Levels

### Super (70B+)
- **Cerebras:** 4 models
- **Groq:** 3 models
- **Mistral:** 5 models
- **Cohere:** 2 models
- **NVIDIA:** 8 models
- **Total:** 22 super models

### Regular (7B-32B)
- **Cerebras:** 2 models
- **Groq:** 5 models
- **Mistral:** 8 models
- **Cohere:** 3 models
- **NVIDIA:** 48 models
- **Total:** 66 regular models

---

## Usage Examples

```python
from juggler import LLMJuggler

juggler = LLMJuggler()

# Use fastest model
response = juggler.juggle(
    messages=[{"role": "user", "content": "Quick question"}],
    preferred_provider="groq",
    preferred_model="llama-3.1-8b-instant"
)

# Use best quality
response = juggler.juggle(
    messages=[{"role": "user", "content": "Complex analysis"}],
    power="super",
    preferred_provider="cerebras",
    preferred_model="qwen-3-235b-a22b-instruct-2507"
)

# Use vision model
response = juggler.juggle(
    messages=[{"role": "user", "content": "Describe this image"}],
    capabilities=["vision"],
    preferred_provider="groq"
)

# Stream response
for chunk in juggler.juggle_stream(
    messages=[{"role": "user", "content": "Tell me a story"}],
    preferred_provider="cerebras"
):
    print(chunk, end='', flush=True)
```

---

**Last Updated:** December 5, 2025  
**Total Models:** 88  
**Streaming Support:** 88/88 (100%)
